---
title: "100-wgs-download"
output: html_document
date: "2023-06-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

```{r}
library(tidyverse)
```

## https://www.ncbi.nlm.nih.gov/bioproject/PRJNA311498

```{r}
sra<-read_csv("meta/lates-wgs-SraRunInfo.csv")
sra %>% select(download_path)
```

Doing this a lazy way.    

in data/raw

```{sh, eval=FALSE}
cat ../../meta/lates-wgs-SraRunInfo.csv | cut -d ',' -f 10 | while read line; do wget $line; done;
```

Need to dump files, something like

fastq-dump --outdir split --skip-technical--readids --read-filter pass --dumpbase --split-3 --gzip --clip 

module load ncbi-toolkit/26_0_1 (added to bash_profile)     
module load sratoolkit/3.0.0       

fastq-dump --outdir split --skip-technical --readids --read-filter pass --dumpbase --split-3 --gzip --clip raw/SRR3165618.sralite.1      

Produces files like: 
SRR3165618.sralite.1_pass_1.fastq.gz      
SRR3165618.sralite.1_pass_2.fastq.gz     

can use doAlign-zipped.sh to align which will give us properly paired and PCR deduplicated read counts. Should probably consider per-site coverage calcs. Adding to doAlign-zipped.sh .      

samtools depth -a $name.bam | awk '{sum+="\$3"} END {print sum/NR}' 

Note to self: Trial on an reduced set of files, say n=1000 fastqs    

## QC     
     
Visualize read counts.        
