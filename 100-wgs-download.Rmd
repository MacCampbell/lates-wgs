---
title: "100-wgs-download"
output: html_document
date: "2023-06-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

```{r}
library(tidyverse)
library(ggrepel)
```

## https://www.ncbi.nlm.nih.gov/bioproject/PRJNA311498

```{r}
sra<-read_csv("meta/lates-wgs-SraRunInfo.csv")
sra %>% select(download_path)
```

Doing this a lazy way.    

in data/raw

```{sh, eval=FALSE}
cat ../../meta/lates-wgs-SraRunInfo.csv | cut -d ',' -f 10 | while read line; do wget $line; done;
```

Need to dump files, something like

fastq-dump --outdir split --skip-technical--readids --read-filter pass --dumpbase --split-3 --gzip --clip 

module load ncbi-toolkit/26_0_1 (added to bash_profile)     
module load sratoolkit/3.0.0       

fastq-dump --outdir split --skip-technical --readids --read-filter pass --dumpbase --split-3 --gzip --clip raw/SRR3165618.sralite.1      

--readids causes bwa to not like it, dropping option
 `for f in raw/*sralite.1; do echo $f; fastq-dump --outdir split --skip-technical --read-filter pass --dumpbase --split-3 --gzip --clip $f; done;  `    

This is going to take a day or so. Next time do something faster!    

Produces files like: 
SRR3165618.sralite.1_pass_1.fastq.gz      
SRR3165618.sralite.1_pass_2.fastq.gz     

can use doAlign-zipped.sh to align which will give us properly paired and PCR deduplicated read counts. Should probably consider per-site coverage calcs. Adding to doAlign-zipped.sh .      

samtools depth -a $name.bam | awk '{sum+="\$3"} END {print sum/NR}' 

Note to self: Trial on an reduced set of files

`(base) maccamp@farm:~/lates-wgs/data/split$ gunzip -c SRR3165592.sralite.1_pass_1.fastq.gz | head -n 400000 > test.sralite.1_pass_1.fastq`      
`(base) maccamp@farm:~/lates-wgs/data/split$ gunzip -c SRR3165592.sralite.1_pass_2.fastq.gz | head -n 400000 > test.sralite.1_pass_2.fastq`      

` gzip test.sralite.1_pass_*`    
`maccamp@farm:~/lates-wgs/data/split$ bash ../../doAlign-zipped.sh test.txt /home/maccamp/lates-wgs/genome/GCF_001640805.2_TLL_Latcal_v3_genomic.fna.gz`      

Ran to completion:   
test-fastqs,200663,97850,0.0151149    

`(base) maccamp@farm:~/lates-wgs/data/split$ ls | grep pass_1 | perl -pe 's/.fastq.gz//g' > forward`     
`(base) maccamp@farm:~/lates-wgs/data/split$ ls | grep pass_2 | perl -pe 's/.fastq.gz//g' > reverse`     
`(base) maccamp@farm:~/lates-wgs/data/split$ ls | grep pass_2 | perl -pe 's/.sralite.1_pass_2.fastq.gz//g' > name`      

(running for 4-10)      
`(base) maccamp@farm:~/lates-wgs/data/split$ paste forward reverse name  > to-align.txt`     
`bash ../../doAlign-zipped.sh to-align.txt /home/maccamp/lates-wgs/genome/GCF_001640805.2_TLL_Latcal_v3_genomic.fna.gz `

Cluster choked. Checking available partitions:

sacctmgr show User $USER --associations
sinfo shows time limits     

bmm is available 150-00:00, trying bmm 7-10:00

Running all jobs!     


At a glance, aligment success is 99.5% or so!


## QC     
     
First, get meta.    

```{r}
meta<-sra %>% select(Run, BioSample, Sample, Region, Locality)
meta$Region<-factor(meta$Region, levels=c("Queensland","Northern Territory","Papua New Guinea",
                                              "Indonesia","Philippines","Vietnam","Cambodia","Thailand",
                                              "India Eastern Coast","India Western Coast"))
meta %>% group_by(Region, Locality) %>% summarize(Count=n())
```
     
```{r}
meta %>% group_by(Region) %>% summarize(Count=n())
```

Visualize read counts.     

```{r}
files<-list.files(path="outputs/100", patter="*.stats", full.names = TRUE)
reads<-lapply(files, read.csv, header=FALSE, col.names=c("Run","Aligned","Filtered","Coverage"))
reads<-bind_rows(reads)
``` 

```{r}
meta2<-left_join(meta,reads)
meta2
```

```{r}
ggplot(meta2) +
  geom_histogram(aes(x=Filtered, fill=Region)) +
  scale_fill_viridis_d(option="turbo") +
  theme_bw() +
  ylab("Count") +
  xlab("Filtered Read Number") +
  theme(panel.grid=element_blank())
```

```{r}
ggplot(meta2) +
  geom_histogram(aes(x=Coverage, fill=Region)) +
  scale_fill_viridis_d(option="turbo") +
  theme_bw() +
  ylab("Count") +
  theme(panel.grid=element_blank())
```

Set up bamlist, maybe downsample? Using doIbs to get random read sampled and a more conventional approach of one region, LG01 with GLs.


```{r}
m2<-meta2 %>% filter(Coverage > 3) %>% mutate(Path=paste0("data/split/",Run,".sort.flt.bam"))
write_tsv(m2 %>% select(Path), file="bamlists/58.bamlist", col_names = FALSE)
m2 %>% group_by(Region) %>% summarize(Count=n())
```

Expectations are that the India fish are divergent, a major mtDNA phylogroup is there. Should probably also find a lesser separation withing of Oz+PNG from Indonesia, Philippnes, Vietnam, Cambodia, Cambodia, Thailand. Though, who knows at this point! Philippines is pretty far away. I may have to try non-allele frequency based methods.    
But still, a MAF of 0.05 should identify fixed variants in India.      

Using LG01/NC_066833.1      

Say, at 75% missing threshold?

```{sh, eval=FALSE}
srun -p high -t 12:00:00 --mem=32G --nodes=1 $HOME/angsd/angsd -P 12  \
-bam bamlists/58.bamlist -r NC_066833.1 \
-ref  genome/GCF_001640805.2_TLL_Latcal_v3_genomic.fna \
-minInd 44 -minMapQ 10 -minQ 20 -GL 2 -doGLF 2 \
-doMajorMinor 1 -doMaf 1 -SNP_pval 1e-6 \
-doIBS 1 -doCounts 1 -doCov 1 -makeMatrix 1 -minMaf 0.05 \
-out outputs/100/58-ibs > outputs/100/58-ibs.out 2> outputs/100/58-ibs.err &
```


	-> Total number of sites analyzed: 24785854
	-> Number of sites retained after filtering: 302009 

Visualize covMat    


```{r}
meta<-m2
m <- as.matrix(read.table("outputs/100/58-ibs.covMat"))
eig <- eigen(m)
var<-eig$values/sum(eig$values)
cumvar<-cumsum(eig$values)/sum(eig$values)

head(var)
head(cumvar)
```


```{r}
covs<-eig$vectors[,1:3] %>% as_tibble() %>% bind_cols(meta)

text12<-covs %>% select(Run, Region, V1, V2) %>%
  group_by(Region) %>% summarize(Count=n(), x=mean(V1), y=mean(V2))

ggplot(covs) +
  geom_point(aes(x=V1, y=V2, fill=Region), pch=21, alpha=0.75) +
  geom_text_repel(data=text12, aes(x=x, y=y, label=Region), max.overlaps = Inf) +
  xlab(paste("PC1", " ", round((100*var[1]),2), "%", sep = "")) +
  ylab(paste("PC2", " ", round((100*var[2]),2), "%", sep = "")) +
  theme_bw() +
  theme(panel.grid=element_blank()) +
  scale_fill_viridis_d(option="turbo")
```

```{r}
text13<-covs  %>% select(Run, Region, V1, V3) %>%
  group_by(Region) %>% summarize(Count=n(), x=mean(V1), y=mean(V3))

ggplot(covs) +
  geom_point(aes(x=V1, y=V3, fill=Region), pch=21, alpha=0.75) +
  geom_text_repel(data=text13, aes(x=x, y=y, label=Region), max.overlaps = Inf) +
  xlab(paste("PC1", " ", round((100*var[1]),2), "%", sep = "")) +
  ylab(paste("PC3", " ", round((100*var[3]),2), "%", sep = "")) +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  scale_fill_viridis_d(option="turbo")
```

## Downsampling

Let's review those counts again:

```{r}
ggplot(meta2) +
  geom_histogram(aes(x=Filtered, fill=Region)) +
  scale_fill_viridis_d(option="turbo") +
  theme_bw() +
  ylab("Count") +
  xlab("Filtered Read Number") +
  theme(panel.grid=element_blank())
```

Downsampling to 5e7 is about 10X coverage or so.
```{r}
median(meta2$Filtered)
meta2 %>% arrange(Filtered) %>% head(n=5)
meta2 %>% filter(Filtered > 5e7) %>% summarize(Count=n())
```


```{r}
meta3<-meta2 %>%  mutate(Frac=5e7/Filtered)  %>% 
  mutate(Path=ifelse(Filtered > 5e7, paste0("data/downsample/",Run,".reduced.bam"),
                     paste0("data/split/",Run,".sort.flt.bam")))
                     
downsample<-meta3 %>% filter(Filtered > 5e7 ) %>%
  mutate(ReductionCommand = paste0("samtools view -bs ",Frac, " ", "/home/maccamp/lates-wgs/data/split/",
                                   Run, ".sort.flt.bam"," > ",
                                   "/home/maccamp/lates-wgs/data/downsample/",
                                   Run,".reduced.bam" )) 

write_csv(downsample$ReductionCommand %>% as_tibble(), "100.1-downsample.sh", col_names = FALSE)
```

Downsampling    
```{sh, eval=FALSE}
srun -p high -t 02:00:00 â€“nodes=1 parallel -j 10 < 100.1-downsample.sh > outputs/100/downsample.stdout 2> outputs/100/downsample.stderr
```

srun -p high -t 12:00:00 --mem=32G --nodes=1 $HOME/angsd/angsd -P 12  \
-bam bamlists/ -r NC_066833.1 \
-minInd   -GL 1 -ref  genome/GCF_001640805.2_TLL_Latcal_v3_genomic.fna.gz -minMaf 0.05 \
-doGLF 2 -doMajorMinor 1 -doMaf 2 -SNP_pval 1e-6 -minMapQ 20 -minQ 20 \
-out outputs/100/ > outputs/302/ -beagle.out 2> outputs/302/ -beagle.err &

srun -p high -t 1:00:00 python $HOME/pcangsd/pcangsd.py -beagle outputs/100/ .beagle.gz -o outputs/100/ -pca -threads 10
```


Covariance matrix from PCAngsd     
